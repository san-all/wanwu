from utils import minio_utils
from utils import es_utils
from utils import mq_rel_utils
from utils import redis_utils
from utils import graph_utils
from utils import knowledge_base_utils

from kafka import KafkaConsumer, TopicPartition, OffsetAndMetadata
import json
import threading
from logging_config import setup_logging

from settings import *

logger_name = 'rag_graph_asyn_add_files_utils'
app_name = os.getenv("LOG_FILE")
logger = setup_logging(app_name, logger_name)
logger.info(logger_name + '---------LOG_FILE：' + repr(app_name))

master_control_logger_name = 'mc_rag_graph_asyn_add_files_utils'
master_control_app_name = os.getenv("LOG_FILE") + "_master_control"
master_control_logger = setup_logging(master_control_app_name, master_control_logger_name)
master_control_logger.info(logger_name + '---------LOG_FILE：' + repr(master_control_app_name))

graph_redis_client = redis_utils.get_redis_connection()


def kafkal():
    while True:
        print('开始消费消息')
        if KAFKA_SASL_USE:
            consumer = KafkaConsumer(KAFKA_GRAPH_TOPICS,
                                     bootstrap_servers=KAFKA_BOOTSTRAP_SERVERS,
                                     security_protocol='SASL_PLAINTEXT',
                                     sasl_mechanism='PLAIN',
                                     sasl_plain_username=KAFKA_SASL_PLAIN_USERNAME,
                                     sasl_plain_password=KAFKA_SASL_PLAIN_PASSWORD,
                                     group_id=KAFKA_GROUP_ID,
                                     enable_auto_commit=KAFKA_ENABLE_AUTO_COMMIT,
                                     max_poll_records=1,  # 设置每次最多拉取1条消息
                                     # max_poll_interval_ms=8000000,  # 设置最大轮询间隔为120分钟
                                     value_deserializer=lambda x: x.decode('utf-8'))

        else:
            consumer = KafkaConsumer(KAFKA_GRAPH_TOPICS,
                                     bootstrap_servers=KAFKA_BOOTSTRAP_SERVERS,
                                     group_id=KAFKA_GROUP_ID,
                                     enable_auto_commit=KAFKA_ENABLE_AUTO_COMMIT,
                                     max_poll_records=1,  # 设置每次最多拉取1条消息
                                     # max_poll_interval_ms=8000000,  # 设置最大轮询间隔为120分钟
                                     value_deserializer=lambda x: x.decode('utf-8'))
        for message in consumer:
            print('收到kafka消息：' + repr(message.value))
            logger.info('收到kafka消息：' + repr(message.value))
            master_control_logger.info('收到kafka消息：' + repr(message.value))
            message_value = json.loads(message.value)

            file_id = message_value["doc"]["id"]
            kb_name = message_value["doc"]["categoryId"]
            user_id = message_value["doc"]["userId"]
            kb_id = message_value["doc"].get("kb_id", "")
            filename = message_value["doc"].get("originalName", "")
            graph_schema_objectname = message_value["doc"].get("graph_schema_objectname", "")
            graph_schema_filename = message_value["doc"].get("graph_schema_filename", "")
            enable_knowledge_graph = message_value["doc"].get("enable_knowledge_graph", "false")
            message_type = message_value["doc"].get("message_type", "graph")
            graph_model_id = message_value["doc"].get("graph_model_id", "")

            try:
                if not KAFKA_ENABLE_AUTO_COMMIT:
                    # 提交当前消息的偏移量
                    tp = TopicPartition(KAFKA_GRAPH_TOPICS, message.partition)
                    offset_and_metadata = OffsetAndMetadata(offset=message.offset + 1, metadata="")
                    offsets = {tp: offset_and_metadata}
                    consumer.commit()
                    logger.info('kafka异步消费完成 ===== 已提交 offset：' + str(message.offset) + '===== kafka消息：' + repr(message.value))
                    master_control_logger.info('kafka异步消费完成 ===== 已提交 offset：' + str(message.offset) + '===== kafka消息：' + repr(message.value))
                    logger.info('consumer.commit offset：' + repr(offsets))
                    master_control_logger.info('consumer.commit offset：' + repr(offsets))

                if KAFKA_USE_ASYN_ADD:
                    # ============ 异步添加 =============
                    lock = threading.Lock()
                    thread = threading.Thread(target=add_files, args=(
                        user_id, kb_name, filename, file_id, enable_knowledge_graph, graph_schema_objectname, graph_schema_filename, graph_model_id, ""))
                    lock.acquire()
                    thread.start()
                    lock.release()
                    # ============ 异步添加 =============
                else:
                    # ============ 顺序添加 =============
                    add_files(user_id, kb_name, filename, file_id, enable_knowledge_graph, graph_schema_objectname, graph_schema_filename, graph_model_id, kb_id=kb_id)
                    # ============ 顺序添加 =============
                logger.info('----->kafka异步消费完成：user_id=%s,kb_name=%s,filename=%s,file_id=%s,process finished' % (user_id, kb_name,filename,file_id))
                master_control_logger.info('----->kafka异步消费完成：user_id=%s,kb_name=%s,filename=%s,file_id=%s,process finished' % (user_id, kb_name, filename, file_id))

            except Exception as e:
                logger.error("kafka处理异常：" + repr(e))
                master_control_logger.error("kafka处理异常：" + repr(e))
                continue



def add_files(user_id, kb_name, file_name, file_id, enable_knowledge_graph, graph_schema_objectname, graph_schema_filename, graph_model_id="", kb_id=""):

    # -------------- 先将从数据库中获取 all_extrac_graph_chunks--------------
    try:
        user_data_path = './user_data'
        filepath = os.path.join(user_data_path, user_id, kb_name)
        logger.info('add_files_filepath=%s' % filepath)
        master_control_logger.info('add_files_filepath=%s' % filepath)
        if not os.path.exists(filepath):
            os.makedirs(filepath)
        else:
            logger.info('filepath=%s 已存在' % filepath)
            master_control_logger.info('filepath=%s 已存在' % filepath)
        all_wait_extrac_chunks = graph_utils.get_all_extrac_graph_chunks(user_id, kb_name, file_name)
        logger.info(repr(file_name) + 'all_wait_extrac_chunks长度：' + repr(len(all_wait_extrac_chunks)))
        master_control_logger.info(repr(file_name) + 'all_wait_extrac_chunks长度：' + repr(len(all_wait_extrac_chunks)))
        logger.info('all_wait_extrac_chunks 获取完成' + "user_id=%s,kb_name=%s,file_name=%s" % (user_id, kb_name, file_name))
        master_control_logger.info('all_wait_extrac_chunks 获取完成' + "user_id=%s,kb_name=%s,file_name=%s" % (user_id, kb_name, file_name))
    except Exception as e:
        import traceback
        logger.error(traceback.format_exc())
        logger.error(repr(e))
        logger.error('all_wait_extrac_chunks 获取失败' + "user_id=%s,kb_name=%s,file_name=%s" % (user_id, kb_name, file_name))
        master_control_logger.error(
            'all_wait_extrac_chunks 获取失败' + "user_id=%s,kb_name=%s,file_name=%s" % (user_id, kb_name, file_name) + repr(e))
        mq_rel_utils.update_doc_status(file_id, status=101)
        return


    # -------------- 将切分好的chunks 进行图谱数据提取构建 --------------
    # 将 chunks 按 batch_size 分组提取
    all_graph_chunks = []
    all_graph_vocabulary_set = set()
    batch_size = 10
    if enable_knowledge_graph == "true":
        schema = {}
        # 当graph_schema_filename,graph_schema_objectname有值则说明用户自己上传excel，否则schema为空后续会用内置schema抽取
        if graph_schema_filename and graph_schema_objectname:
            try:
                schema_file_path = os.path.join(filepath, graph_schema_filename)
                graph_download_status, graph_download_link = minio_utils.get_file_from_minio(graph_schema_objectname,
                                                                                             schema_file_path)
                logger.info("graph_download_status=%s,graph_download_link=%s" %
                            (graph_download_status, graph_download_link))
                schema = graph_utils.parse_excel_to_schema_json(schema_file_path)
                logger.info(f'提取graph schema成功'
                             + "user_id=%s,kb_name=%s,file_name=%s" % (user_id, kb_name, file_name) + str(schema))
                master_control_logger.info(f'提取graph schema成功'
                    + "user_id=%s,kb_name=%s,file_name=%s" % (user_id, kb_name, file_name))
            except Exception as e:
                logger.error(repr(e))
                logger.error(f'提取graph schema失败'
                             + "user_id=%s,kb_name=%s,file_name=%s" % (user_id, kb_name, file_name))
                master_control_logger.error(f'提取graph schema失败'
                                            + "user_id=%s,kb_name=%s,file_name=%s" % (user_id, kb_name, file_name) + repr(e))
                mq_rel_utils.update_doc_status(file_id, status=104)
                return

        extracted_graph_datas = []
        for i in range(0, len(all_wait_extrac_chunks), batch_size):
            batch_num = int(i/batch_size) + 1
            temp_chunks = all_wait_extrac_chunks[i:i + batch_size]
            try:
                result_data = graph_utils.get_extrac_graph_data(user_id, kb_name, temp_chunks, file_name, schema=schema)
                graph_chunks = result_data['graph_chunks']
                all_graph_chunks.extend(graph_chunks)
                all_graph_vocabulary_set.update(result_data['graph_vocabulary_set'])
                for data in graph_chunks:
                    extracted_graph_datas.append(data["graph_data"])
                logger.info(f'第{batch_num}批文档提取graph数据成功'
                             + "user_id=%s,kb_name=%s,file_name=%s" % (user_id, kb_name, file_name) + str(result_data))
                master_control_logger.info(f'第{batch_num}批文档提取graph数据成功'
                    + "user_id=%s,kb_name=%s,file_name=%s" % (user_id, kb_name, file_name))
            except Exception as e:
                logger.error(repr(e))
                logger.error(f'第{batch_num}批文档提取graph数据失败'
                             + "user_id=%s,kb_name=%s,file_name=%s" % (user_id, kb_name, file_name))
                master_control_logger.error(f'第{batch_num}批文档提取graph数据失败'
                    + "user_id=%s,kb_name=%s,file_name=%s" % (user_id, kb_name, file_name) + repr(e))
                mq_rel_utils.update_doc_status(file_id, status=102)
                return

        # if extracted_graph_datas:
        #     try:
        #         reports_result = knowledge_base_utils.generate_community_reports(kb_name, file_name, extracted_graph_datas)
        #         reports = reports_result['community_reports']
        #         old_chunk_total_num = len(chunks)
        #         chunk_current_num = old_chunk_total_num + 1
        #         new_chunk_total_num = old_chunk_total_num + len(reports)
        #         for chunk in chunks:
        #             meta_data = copy.deepcopy(chunk["meta_data"])
        #             meta_data["chunk_total_num"] = new_chunk_total_num
        #             chunk["meta_data"] = meta_data
        #         # 更新subchunk 会影响
        #         # for chunk in sub_chunk:
        #         #     chunk["meta_data"]["chunk_total_num"] = new_chunk_total_num
        #         for report_data in reports:
        #             chunks.append({
        #                "text": report_data["report"],
        #                 "meta_data": {
        #                     "file_name": file_name,
        #                     "entities": report_data["entities"],
        #                     "chunk_total_num": new_chunk_total_num,
        #                     "chunk_current_num": chunk_current_num
        #                 },
        #                 "chunk_type": "community_report"
        #             })
        #             chunk_current_num += 1
        #         logger.info(f"文档提取社区报告成功, user_id=%s,kb_name=%s,file_name=%s" % (user_id, kb_name, file_name))
        #         master_control_logger.info(f"文档提取社区报告成功, user_id=%s,kb_name=%s,file_name=%s" % (user_id, kb_name, file_name))
        #     except Exception as e:
        #         logger.error(repr(e))
        #         logger.error(f"文档提取社区报告失败, user_id=%s,kb_name=%s,file_name=%s" % (user_id, kb_name, file_name))
        #         master_control_logger.error(f"文档提取社区报告失败, user_id=%s,kb_name=%s,file_name=%s" % (user_id, kb_name, file_name) + repr(e))

    # --------------  insert es graph_data ----------------
    try:
        if enable_knowledge_graph == "true" and len(all_graph_chunks) > 0:
            logger.info(f'graph_data 插入es开始,all_graph_chunks len:{len(all_graph_chunks)}')
            master_control_logger.info(f'graph_data 插入es开始,all_graph_chunks len:{len(all_graph_chunks)}')
            insert_es_result = es_utils.add_es(user_id, kb_name, all_graph_chunks, file_name, kb_id=kb_id)
            logger.info(repr(file_name) + '添加es结果：' + repr(insert_es_result))
            master_control_logger.info(repr(file_name) + '添加es结果：' + repr(insert_es_result))
            if insert_es_result['code'] != 0:
                # 回调
                logger.error('graph_data插入es失败' + "user_id=%s,kb_name=%s,file_name=%s" % (user_id, kb_name, file_name))
                master_control_logger.error(
                    'graph_data插入es失败' + "user_id=%s,kb_name=%s,file_name=%s" % (user_id, kb_name, file_name))
                mq_rel_utils.update_doc_status(file_id, status=103)
                return
            else:
                # 插入成功后，更新update_graph_vocabulary_set 数据
                kb_id = knowledge_base_utils.get_kb_name_id(user_id, kb_name)
                redis_utils.update_graph_vocabulary_set(graph_redis_client, kb_id,
                                                        elements_to_add=all_graph_vocabulary_set)
                # 回调
                logger.info('graph_data插入es完成' + "user_id=%s,kb_name=%s,file_name=%s" % (user_id, kb_name, file_name))
                master_control_logger.info(
                    'graph_data插入es完成' + "user_id=%s,kb_name=%s,file_name=%s" % (user_id, kb_name, file_name))
    except Exception as e:
        logger.error(repr(e))
        logger.error('graph_data插入es失败' + "user_id=%s,kb_name=%s,file_name=%s" % (user_id, kb_name, file_name))
        master_control_logger.error(
            'graph_data插入es失败' + "user_id=%s,kb_name=%s,file_name=%s" % (user_id, kb_name, file_name) + repr(e))
        mq_rel_utils.update_doc_status(file_id, status=103)
        return

    # --------------7、最终完成
    # 回调
    logger.info("user_id=%s,kb_name=%s,file_name=%s" % (user_id, kb_name, file_name) + '===== 文档grahp解析成功且完成')
    master_control_logger.info("user_id=%s,kb_name=%s,file_name=%s,kb_id=%s" % (user_id, kb_name, file_name, kb_id) + '===== 文档grahp解析成功且完成')
    mq_rel_utils.update_doc_status(file_id, status=100)


if __name__ == "__main__":
    kafkal()
